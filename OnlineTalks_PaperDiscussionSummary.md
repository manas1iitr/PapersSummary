1. [Contrastive Unsupervised Representations for Reinforcement Learning](https://www.youtube.com/watch?v=1MprzvYNpY8) Self-supervised learning can now perform better than supervised learning on the same tasks as well. The key ingredient in terms of model size, data, and training time for self-supervised pretraining seems to be the training time. For example, self-supervised pre training is sometimes done for 1000 epochs and then fine-tuned, while supervised learning for a task is generally done for 100-200 epochs. It has also been shown that training the supervised models for 1000 epochs does not match the performance of the self-supervised pre-training. The ****key*** thing behind the recent success of self-supervised pre-training has been the contrastive loss alongwith data augmentation (random cropping, adding random noise/color) in images, it is not yet clear how this could be done in text (thought the inspiration for this comes from skip gram word2vec but there is no as such 'data augmentation there'). The contrastive loss seems to be much more effective in providing more useful bits of information than the reconstruction loss (though one may argue that the reconstruction may provide more bits of information but the counter is that are all the bits provided in that case are useful). Also, unsupervised pretraining seems to be much more effective at transfer learning as well. Key papers to look at are MOCO (FAIR-Kaiming), SimCLR and SimCLRv2 (Google-Hinton), CURL (DeepMind), Contrastive Predictive Coding (CPC) and CPCv2 (Google). CNN seem to be have the right kind of inductive bias for all different kind of modality like audio, speech, image, sometimes text, RL, but they are limited by their local structure which liimits the global message passing. Transformer is another architecture with good inductive bias, but how to scale them to images, we cannot make every pixel attend to everything else. That is why research on lightweight transformers is important. Also, self-attention GANs to be read.
