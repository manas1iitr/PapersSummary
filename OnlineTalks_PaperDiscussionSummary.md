1. [Contrastive Unsupervised Representations for Reinforcement Learning](https://www.youtube.com/watch?v=1MprzvYNpY8) Self-supervised learning (with an added linear classifer which is trained with all the pretrained layers frozen) can now perform better than supervised learning on the same tasks as well. **The key ingredient out of model size, data, and training time for self-supervised pretraining seems to be the training time.** For example, self-supervised pre training is sometimes done for 1000 epochs and then fine-tuned (WITH ANY AMOUNT OF LABELS, BE IT 5% of IMAGENET OR BE IT 10% of IMAGENET), while supervised learning for a task is generally done for 100-200 epochs. It has also been shown that training the supervised models for 1000 epochs does not match the performance of the self-supervised pre-training (WHICH IS OUTRAGEOUS AND NON-INTUITIVE now, but earlier before batchnorm and all, we were doing layerwise pretraining). The ****key*** thing behind the recent success of self-supervised pre-training has been the contrastive loss alongwith data augmentation (random cropping, adding random noise/color) in images, it is not yet clear how this could be done in text (thought the inspiration for this comes from skip gram word2vec but there is no as such 'data augmentation there'). The contrastive loss seems to be much more effective in providing more useful bits of information than the reconstruction loss (though one may argue that the reconstruction may provide more bits of information but the counter is that are all the bits provided in that case are useful). Also, ***unsupervised pretraining seems to provide a much more effective backbone for transfer learning as well than the supervised trained model***. Key papers to look at are MOCO (FAIR-Kaiming), SimCLR and SimCLRv2 (Google-Hinton), CURL (DeepMind), Contrastive Predictive Coding (CPC) and CPCv2 (Google). CNN seem to be have the right kind of inductive bias for all different kind of modality like audio, speech, image, sometimes text, RL, but they are limited by their local structure which liimits the global message passing. Transformer is another architecture with good inductive bias, but how to scale them to images, we cannot make every pixel attend to everything else. That is why research on lightweight transformers is important. Also, self-attention GANs to be read.
***Yann Le Cunn on Self-Supervised Learning:*** I now call it "self-supervised learning", because "unsupervised" is both a loaded and confusing term. In self-supervised learning, the system learns to predict part of its input from other parts of it input. In other words a portion of the input is used as a supervisory signal to a predictor fed with the remaining portion of the input.  Self-supervised learning uses way more supervisory signals than supervised learning, and enormously more than reinforcement learning. That's why calling it "unsupervised" is totally misleading. That's also why more knowledge about the structure of the world can be learned through self-supervised learning than from the other two paradigms: the data is unlimited, and amount of feedback provided by each example is huge. Self-supervised learning has been enormously successful in natural language processing. For example, the BERT model and similar techniques produce excellent representations of text. BERT is a prototypical example of self-supervised learning: show it a sequence of words on input, mask out 15% of the words, and ask the system to predict the missing words (or a distribution of words). This an example of masked auto-encoder, itself a special case of denoising auto-encoder, itself an example of self-supervised learning based on reconstruction or prediction. But text is a discrete space in which probability distributions are easy to represent. So far, similar approaches haven't worked quite as well for images or videos because of the difficulty of representing distributions over high-dimensional continuous spaces. Doing this properly and reliably is the greatest challenge in ML and AI of the next few years in my opinion.


2. [RepNet](http://openaccess.thecvf.com/content_CVPR_2020/papers/Dwibedi_Counting_Out_Time_Class_Agnostic_Video_Repetition_Counting_in_the_CVPR_2020_paper.pdf)       
[Yannic's video link](https://www.youtube.com/watch?v=qSArFEIoSbo) Paper talks about identifying an action and counting its periodicity in a video (think: jumping jack or pushups), has some cool architecture choices and nice experiments (ref fig 7) showing the model actually does what is claimed, also has some nice ablation studies. So basically the model goes like this, a video has some F frames, each of these is fed through a Resnet50 to get a feature map, then 3D temporal convolutions are performed on them so that each frame gets knowledge about its previous and next frame, and finally max-pooling is done to get some d(=512) dimensional vector. Then THE CLEVER, INTELLIGENT CHOICE IN THE ARCHITECTURE IS MADE OF INTRODUCING A TEMPORAL SELF-SIMILARITY MATRIX, which can capture how similar ith frame is to the jth frame (something like cosine similarity, though they actually use L2 norm on i-j vector). Then this matrix can be treated as an image, which is upsampled (passed through a convolution layer to get multiple channels), which is then passed through a transformer to capture contextual representation of each frame which is then passed through a feed forward and convolution layers to finally get the prediction. 
They also use an interesting technique to create a synthetic dataset (as not many datasets exist for this task), we have a video with frame f1,f2,f3,....,fn-2,fn-1,fn. Let us modify this into f1,f2,(f3,f4,f5,..,fn-2,fn-3,fn-4,..,f3)(the bracket term repeated multiple times),fn-1,fn. This gives us a video containing periodic motion of anything, and we can use this for training. ***BOTTOMLINE: THIS PAPER TEACHES SHOWS THAT MODEL CAN BE TRAINED TO LEARN THE SKILL OF COUNTING WHICH HUMANS ARE INNATELY BORN WITH.***
