1. [Contrastive Unsupervised Representations for Reinforcement Learning](https://www.youtube.com/watch?v=1MprzvYNpY8) Self-supervised learning can now perform better than supervised learning on the same tasks as well. The key ingredient in terms of model size, data, and training time for self-supervised pretraining seems to be the training time. For example, self-supervised pre training is sometimes done for 1000 epochs and then fine-tuned, while supervised learning for a task is generally done for 100-200 epochs. It has also been shown that training the supervised models for 1000 epochs does not match the performance of the self-supervised pre-training (WHICH IS OUTRAGEOUS AND NON-INTUITIVE now, but earlier before batchnorm and all, we were doing layerwise pretraining). The ****key*** thing behind the recent success of self-supervised pre-training has been the contrastive loss alongwith data augmentation (random cropping, adding random noise/color) in images, it is not yet clear how this could be done in text (thought the inspiration for this comes from skip gram word2vec but there is no as such 'data augmentation there'). The contrastive loss seems to be much more effective in providing more useful bits of information than the reconstruction loss (though one may argue that the reconstruction may provide more bits of information but the counter is that are all the bits provided in that case are useful). Also, unsupervised pretraining seems to be much more effective at transfer learning as well. Key papers to look at are MOCO (FAIR-Kaiming), SimCLR and SimCLRv2 (Google-Hinton), CURL (DeepMind), Contrastive Predictive Coding (CPC) and CPCv2 (Google). CNN seem to be have the right kind of inductive bias for all different kind of modality like audio, speech, image, sometimes text, RL, but they are limited by their local structure which liimits the global message passing. Transformer is another architecture with good inductive bias, but how to scale them to images, we cannot make every pixel attend to everything else. That is why research on lightweight transformers is important. Also, self-attention GANs to be read.
Yann Le Cunn on Self-Supervised Learning: I now call it "self-supervised learning", because "unsupervised" is both a loaded and confusing term. In self-supervised learning, the system learns to predict part of its input from other parts of it input. In other words a portion of the input is used as a supervisory signal to a predictor fed with the remaining portion of the input.  Self-supervised learning uses way more supervisory signals than supervised learning, and enormously more than reinforcement learning. That's why calling it "unsupervised" is totally misleading. That's also why more knowledge about the structure of the world can be learned through self-supervised learning than from the other two paradigms: the data is unlimited, and amount of feedback provided by each example is huge. Self-supervised learning has been enormously successful in natural language processing. For example, the BERT model and similar techniques produce excellent representations of text. BERT is a prototypical example of self-supervised learning: show it a sequence of words on input, mask out 15% of the words, and ask the system to predict the missing words (or a distribution of words). This an example of masked auto-encoder, itself a special case of denoising auto-encoder, itself an example of self-supervised learning based on reconstruction or prediction. But text is a discrete space in which probability distributions are easy to represent. So far, similar approaches haven't worked quite as well for images or videos because of the difficulty of representing distributions over high-dimensional continuous spaces. Doing this properly and reliably is the greatest challenge in ML and AI of the next few years in my opinion.
